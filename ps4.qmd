---
title: "Data Vis PSet 4"
author: "Arianna Wooten"
date: "February 7, 2026"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 02/07 at 5:00PM Central.**

"This submission is my work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: AW \*\*\_\_\*\*

### Github Classroom Assignment Setup and Submission Instructions

1.  **Accepting and Setting up the PS4 Assignment Repository**
    -   Each student must individually accept the repository for the problem set from Github Classroom ("ps4") -- <https://classroom.github.com/a/hWhtcHqH>
        -   You will be prompted to select your cnetid from the list in order to link your Github account to your cnetid.
        -   If you can't find your cnetid in the link above, click "continue to next step" and accept the assignment, then add your name, cnetid, and Github account to this Google Sheet and we will manually link it: <https://rb.gy/9u7fb6>
    -   If you authenticated and linked your Github account to your device, you should be able to clone your PS4 assignment repository locally.
    -   Contents of PS4 assignment repository:
        -   `ps4_template.qmd`: this is the Quarto file with the template for the problem set. You will write your answers to the problem set here.
2.  **Submission Process**:
    -   Knit your completed solution `ps4.qmd` as a pdf `ps4.pdf`.
        -   Your submission does not need runnable code. Instead, you will tell us either what code you ran or what output you got.
    -   To submit, push `ps4.qmd` and `ps4.pdf` to your PS4 assignment repository. Confirm on Github.com that your work was successfully pushed.

### Grading
- You will be graded on what was last pushed to your PS4 assignment repository before the assignment deadline
- Problem sets will be graded for completion as: {missing (0%); ✓- (incomplete, 50%); ✓+ (excellent, 100%)}
    - The percent values assigned to each problem denote how long we estimate the problem will take as a share of total time spent on the problem set, not the points they are associated with.
- In order for your submission to be considered complete, you need to push both your `ps4.qmd` and `ps4.pdf` to your repository. Submissions that do not include both files will automatically receive 50% credit.


\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

Reading in the data and parsing with BeautifulSoup: 
```{python}
import requests 

# read in the web page
url = 'https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)

text = response.text
print(text) #looks right

# parse the HTML code with beautifulsoup
from bs4 import BeautifulSoup
soup = BeautifulSoup(text, 'lxml')

print(soup)
```

After manually inspecting the site's HTML code, I think the title is <a> (nested in <h2>, nested in <header>, nested in <div>), the date is <span>, the category is <li> nested in <ul>, and the link is <a> (nested inside of the title tag). 

The data I want starts about halfway into the HTML code under the tag <main>.
Desired tag ranges (e.g. I want the 82nd <a> tag through the 126th):
<a> tag: 82 - ? 
  7 - ? in <main>
<span> tag: 28 - ?
<li> tag: 90 - ?



```{python}
# Subset the HTML data to the <main> tag because all the data is nested within "main" and I want to exclude code for the site's headers, footers, etc.
main = soup.find('main')

# Use .find_all to find all the tags mentioned above.
# These variables are messy and return code that includes more than just the text of the data that I want.
titles = main.find_all('a') # currently same as links
categories = main.find_all('ul')
links = main.find_all('a') # currently same as titles
dates = main.find_all('span')
```

```{python}
# Find the first tag that I want for each and see the output:

titles[5] # output: <a href="/fraud/enforcement/houston-transplant-doctor-indicted-for-making-false-statements-in-patients-medical-records/">Houston Transplant Doctor Indicted For Making False Statements In Patients’ Medical Records</a>

categories[1] # super long output

links[5] # <a href="/fraud/enforcement/houston-transplant-doctor-indicted-for-making-false-statements-in-patients-medical-records/">Houston Transplant Doctor Indicted For Making False Statements In Patients’ Medical Records</a>

dates[14] # <span class="text-base-dark padding-right-105">February 5, 2026</span>

```
```{python}
# For each data type, figure out how to extract the part of the tag that I want

title1 = titles[5].text 
title1 # returns the title

cat1 = categories[2].find('li').text
cat1

link1 = links[5].get('href')
link1 #not the full link yet

date1 = dates[14].text
date1

```

```{python}
# Use for loops to put all of the data into lists

data_titles = []
for i in titles[5:]: # start loop at titltes[6]
  data_titles.append([i.text]) # add it to data_titles

data_cats = []
for i in categories[2:]: 
  text = i.find('li').text
  if text.startswith(('Child', 'CIA', 'CMP', 'COVID', 'Criminal', 'EMTALA', 'Fraud', 'Grant', 'State', 'Stipulated')): # source: https://www.w3schools.com/python/ref_string_startswith.asp
    data_cats.append([text]) 
  
data_links = []
for i in links[5:]: 
  link = f"https://oig.hhs.gov{i.get('href')}"
  data_links.append([link]) 

data_dates = []
for i in dates[14:]: 
  data_dates.append([i.text]) 

```
```{python}
# get rid of any weird accidental values and make sure the data can line up as rows
data_titles = data_titles[0:20] #20 rows

data_cats # 20 rows, but I think it's only including the first category; some rows have multiple

data_links = data_links[0:20] # 20 rows

data_dates # 20 rows

assert len(data_titles) == len(data_cats) == len(data_links) == len(data_dates), 'different lengths'

# create the dataframe

df = pd.DataFrame({'title': data_titles, 'category': data_cats, 'link': data_links, 'date': data_dates})

df

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code


* b. Create Dynamic Scraper

```{python}



```

* c. Test Your Code

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time

```{python}

```

### 2. Plot the number of enforcement actions categorized:

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```
